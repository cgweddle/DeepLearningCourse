{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_hw2p2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qi9TCCFI7dYs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as ttf\n",
        "\n",
        "import csv\n",
        "import math\n",
        "\n",
        "import os\n",
        "import os.path as osp\n",
        "\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/home/ubuntu/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('{\"username\":\"chrisweddle\",\"key\":\"504463625d95239beccef5da3e740f6d\"}') # Put your kaggle username & key here"
      ],
      "metadata": {
        "id": "MkPugrnL7iRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c 11-785-s22-hw2p2-classification\n",
        "!kaggle competitions download -c 11-785-s22-hw2p2-verification\n",
        "\n",
        "!unzip -o 11-785-s22-hw2p2-classification.zip\n",
        "!unzip -o 11-785-s22-hw2p2-verification.zip\n"
      ],
      "metadata": {
        "id": "4NoCzpDH7jZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The well-accepted SGD batch_size & lr combination for CNN classification is 256 batch size for 0.1 learning rate.\n",
        "When changing batch size for SGD, follow the linear scaling rule - halving batch size -> halve learning rate, etc.\n",
        "This is less theoretically supported for Adam, but in my experience, it's a decent ballpark estimate.\n",
        "\"\"\"\n",
        "#batch_size = 256\n",
        "#lr = 0.1\n",
        "#batch_size=128\n",
        "#lr=.05\n",
        "batch_size=64\n",
        "#lr = 0.025\n",
        "lr = 0.0185\n",
        "#batch_size = 32\n",
        "#lr = 0.012\n",
        "epochs = 20 # Just for the early submission. We'd want you to train like 50 epochs for your main submissions."
      ],
      "metadata": {
        "id": "-_eWfSMb7kar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InvertedResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Intuitively, layers in MobileNet can be split into \"feature mixing\" \n",
        "    and \"spatial mixing\" layers. You can think of feature mixing as each pixel\n",
        "    \"thinking on its own\" about its own featuers, and you can think of spatial\n",
        "    mixing as pixels \"talking with each other\". Alternating these two builds\n",
        "    up a CNN.\n",
        "\n",
        "    In a bit more detail:\n",
        "\n",
        "    - The purpose of the \"feature mixing\" layers is what you've already seen in \n",
        "    hw1p2. Remember, in hw1p2, we went from some low-level audio input to\n",
        "    semantically rich representations of phonemes. Featuring mixing is simply a \n",
        "    linear layer (a weight matrix) that transforms simpler features into \n",
        "    something more advanced.\n",
        "\n",
        "    - The purpose of the \"spatial mixing\" layers is to mix features from different\n",
        "    spatial locations. You can't figure out a face by looking at each pixel on\n",
        "    its own, right? So we need 3x3 convolutions to mix features from neighboring\n",
        "    pixels to build up spatially larger features.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 stride,\n",
        "                 expand_ratio):\n",
        "        super().__init__() # Just have to do this for all nn.Module classes\n",
        "\n",
        "        # Can only do identity residual connection if input & output are the\n",
        "        # same channel & spatial shape.\n",
        "        if stride == 1 and in_channels == out_channels:\n",
        "            self.do_identity = True\n",
        "        else:\n",
        "            self.do_identity = False\n",
        "        \n",
        "        # Expand Ratio is like 6, so hidden_dim >> in_channels\n",
        "        hidden_dim = in_channels * expand_ratio\n",
        "\n",
        "        \"\"\"\n",
        "        Also, note that bias = False since BatchNorm2d has a bias term built-in.\n",
        "\n",
        "        As you go, note the relationship between kernel_size and padding. As you\n",
        "        covered in class, padding = kernel_size // 2 (kernel_size being odd) to\n",
        "        make sure input & output spatial resolution is the same.\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        Change in_channels to in_channels, out_channels to hidden_dim\n",
        "        Change activation to GELU()\n",
        "        \"\"\"\n",
        "        self.feature_mixing = nn.Sequential(\n",
        "            # TODO: Fill this in!\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=hidden_dim, kernel_size = 1, stride = 1, padding = 1//2, bias=True),\n",
        "            nn.GELU()\n",
        "\n",
        "        )\n",
        "\n",
        "        \"\"\"\n",
        "        For Convnext: Change kernel_size to 7, in_channels and out_channels from hidden_dim to in_channels, padding to 7//2\n",
        "        Get rid of activation\n",
        "        \"\"\"\n",
        "        self.spatial_mixing = nn.Sequential(\n",
        "            # TODO: Fill this in!\n",
        "            nn.Conv2d(in_channels = in_channels, out_channels = in_channels, kernel_size = 7, stride = stride, groups=in_channels, padding = 7//2, bias=False),\n",
        "            nn.BatchNorm2d(in_channels)\n",
        "        )\n",
        "\n",
        "        \"\"\"\n",
        "        What's this? Remember that hidden_dim is quite large - six times the \n",
        "        in_channels. So it was nice to do the above operations in this high-dim\n",
        "        space, where some patterns might be more clear. But we still want to \n",
        "        bring it back down-to-earth.\n",
        "\n",
        "        Intuitively, you can takeaway two reasons for doing this:\n",
        "        - Reduces computational cost by a lot. 6x in & out channels means 36x\n",
        "          larger weights, which is crazy. We're okay with just one of input or \n",
        "          output of a convolutional layer being large when mixing channels, but \n",
        "          not both.\n",
        "        \n",
        "        - We also want a residual connection from the input to the output. To \n",
        "          do that without introducing another convolutional layer, we want to\n",
        "          condense the # of channels back to be the same as the in_channels.\n",
        "          (out_channels and in_channels are usually the same).\n",
        "        \"\"\"\n",
        "        self.bottleneck_channels = nn.Sequential(\n",
        "            # TODO: Fill this in!\n",
        "            nn.Conv2d(in_channels = hidden_dim, out_channels = out_channels, kernel_size=1, stride=1, padding=0, bias=True)\n",
        "            #nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "    \"\"\"\n",
        "    For convnext: switch the spatial_mixing and feature_mixing layers\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "        #out = self.feature_mixing(x)\n",
        "        out = self.spatial_mixing(x)\n",
        "        out = self.feature_mixing(out)\n",
        "        out = self.bottleneck_channels(out)\n",
        "\n",
        "        if self.do_identity:\n",
        "            return x + out\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "class Convnext(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_classes= 7000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        For convnext: Change out_channels to 96, kernel_size to 4\n",
        "        \"\"\"\n",
        "        self.stem = nn.Sequential(\n",
        "            # TODO: Fill this in!\n",
        "            \n",
        "            #nn.Conv2d(in_channels=3, out_channels = 96, kernel_size = 4, stride=4, padding = 4//2, bias=False),\n",
        "            nn.Conv2d(in_channels=3, out_channels = 32, kernel_size = 3, stride=2, padding = 3//2, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.GELU()\n",
        "            #nn.ReLU6()\n",
        "        )\n",
        "\n",
        "        \"\"\"\n",
        "        Since we're just repeating InvertedResidualBlocks again and again, we\n",
        "        want to specify their parameters like this.\n",
        "        The four numbers in each row (a stage) are shown below.\n",
        "        - Expand ratio: We talked about this in InvertedResidualBlock\n",
        "        - Channels: This specifies the channel size before expansion\n",
        "        - # blocks: Each stage has many blocks, how many?\n",
        "        - Stride of first block: For some stages, we want to downsample. In a\n",
        "          downsampling stage, we set the first block in that stage to have\n",
        "          stride = 2, and the rest just have stride = 1.\n",
        "\n",
        "        Again, note that almost every stage here is downsampling! By the time\n",
        "        we get to the last stage, what is the image resolution? Can it still\n",
        "        be called an image for our dataset? Think about this, and make changes\n",
        "        as you want.\n",
        "        \"\"\"\n",
        "        self.stage_cfgs = [\n",
        "            # expand_ratio, channels, # blocks, stride of first block\n",
        "            [6,96,3,2],\n",
        "            [8,192,3,2],\n",
        "            [8,384,7,2],\n",
        "            [8,512,3,2]\n",
        "        ]\n",
        "\n",
        "        # Remember that our stem left us off at 16 channels. We're going to \n",
        "        # keep updating this in_channels variable as we go\n",
        "        in_channels = 32\n",
        "\n",
        "        # Let's make the layers\n",
        "        layers = []\n",
        "        for curr_stage in self.stage_cfgs:\n",
        "            expand_ratio, num_channels, num_blocks, stride = curr_stage\n",
        "            \n",
        "            for block_idx in range(num_blocks):\n",
        "                out_channels = num_channels\n",
        "                layers.append(InvertedResidualBlock(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    # only have non-trivial stride if first block\n",
        "                    stride=stride if block_idx == 0 else 1, \n",
        "                    expand_ratio=expand_ratio\n",
        "                ))\n",
        "                # In channels of the next block is the out_channels of the current one\n",
        "                in_channels = out_channels \n",
        "            \n",
        "        self.layers = nn.Sequential(*layers) # Done, save them to the class\n",
        "\n",
        "        # Some final feature mixing\n",
        "        self.final_block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 1280, kernel_size=1, padding=0, stride=1, bias=False),\n",
        "            nn.BatchNorm2d(1280),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        # Now, we need to build the final classification layer.\n",
        "        self.cls_layer = nn.Sequential(\n",
        "            # TODO: Fill this in!\n",
        "            #nn.AvgPool2d((7,7)),\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "            nn.Flatten(),\n",
        "            #nn.Linear(1280, num_classes)\n",
        "            nn.Linear(1280, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU6(),\n",
        "            nn.Linear(512, num_classes)\n",
        "\n",
        "            \n",
        "            # Pool over & collapse the spatial dimensions to (1, 1)\n",
        "            # Collapse the trivial (1, 1) dimensions\n",
        "            # Project to our # of classes\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"\n",
        "        Usually, I like to use default pytorch initialization for stuff, but\n",
        "        MobileNetV2 made a point of putting in some custom ones, so let's just\n",
        "        use them.\n",
        "        \"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x, return_feats=False):\n",
        "        out = self.stem(x)\n",
        "        out = self.layers(out)\n",
        "        out = self.final_block(out)\n",
        "        if return_feats:\n",
        "            return out\n",
        "        out = self.cls_layer(out)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "ltcoVmvp7nFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Transforms (data augmentation) is quite important for this task.\n",
        "Go explore https://pytorch.org/vision/stable/transforms.html for more details\n",
        "\"\"\"\n",
        "DATA_DIR = \"/home/ubuntu\"\n",
        "#TRAIN_DIR = osp.join(DATA_DIR, \"train_subset/train_subset\") # This is a smaller subset of the data. Should change this to classification/classification/train\n",
        "TRAIN_DIR = osp.join(DATA_DIR, \"classification/classification/train\")\n",
        "VAL_DIR = osp.join(DATA_DIR, \"classification/classification/dev\")\n",
        "TEST_DIR = osp.join(DATA_DIR, \"classification/classification/test\")\n",
        "\n",
        "#brightnes and contast: .6 - 1.4\n",
        "train_transforms = [ttf.RandomAffine(5),\n",
        "                    ttf.RandomApply(nn.ModuleList([ttf.ColorJitter(brightness=(.6,1.4), contrast=(.6,1.4))]), p=0.5), \n",
        "                    ttf.RandomHorizontalFlip(p=0.5), ttf.RandomAdjustSharpness(0, p=0.5),ttf.RandomAutocontrast(p=0.5),\n",
        "                   ttf.ToTensor()]\n",
        "val_transforms = [ttf.ToTensor()]\n",
        "\n",
        "train_dataset = torchvision.datasets.ImageFolder(TRAIN_DIR,\n",
        "                                                 transform=ttf.Compose(train_transforms))\n",
        "val_dataset = torchvision.datasets.ImageFolder(VAL_DIR,\n",
        "                                               transform=ttf.Compose(val_transforms))\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                          shuffle=True, drop_last=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                        drop_last=True, num_workers=4)"
      ],
      "metadata": {
        "id": "qFheSzAG7pLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = Network()\n",
        "model = Convnext()\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.cuda()\n",
        "\n",
        "# For this homework, we're limiting you to 35 million trainable parameters, as\n",
        "# outputted by this. This is to help constrain your search space and maintain\n",
        "# reasonable training times & expectations\n",
        "num_trainable_parameters = 0\n",
        "for p in model.parameters():\n",
        "    num_trainable_parameters += p.numel()\n",
        "print(\"Number of Params: {}\".format(num_trainable_parameters))\n",
        "\n",
        "# TODO: What criterion do we use for this task?\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=.1)\n",
        "#criterion = LabelSmoothingCrossEntropy(label_smoothing=.1)\n",
        "#criterion = LabelSmoothing(smoothing = 0.2)\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(train_loader) * epochs))\n",
        "# T_max is \"how many times will i call scheduler.step() until it reaches 0 lr?\"\n",
        "\n",
        "# For this homework, we strongly strongly recommend using FP16 to speed up training.\n",
        "# It helps more for larger models.\n",
        "# Go to https://effectivemachinelearning.com/PyTorch/8._Faster_training_with_mixed_precision\n",
        "# and compare \"Single precision training\" section with \"Mixed precision training\" section\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ],
      "metadata": {
        "id": "JnQvkemX7pOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val = 0\n",
        "#model.load_state_dict(torch.load('/home/ubuntu/model_state.pth'))\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Quality of life tip: leave=False and position=0 are needed to make tqdm usable in jupyter\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "\n",
        "    num_correct = 0\n",
        "    total_loss = 0\n",
        "    val_num_correct = 0\n",
        "\n",
        "    for i, (x, y) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        x = x.cuda()\n",
        "        y = y.cuda()\n",
        "\n",
        "        # Don't be surprised - we just wrap these two lines to make it work for FP16\n",
        "        with torch.cuda.amp.autocast():     \n",
        "            outputs = model(x)\n",
        "            loss = criterion(outputs, y)\n",
        "\n",
        "        # Update # correct & loss as we go\n",
        "        num_correct += int((torch.argmax(outputs, axis=1) == y).sum())\n",
        "        total_loss += float(loss)\n",
        "\n",
        "        # tqdm lets you add some details so you can monitor training as you train.\n",
        "        batch_bar.set_postfix(\n",
        "            acc=\"{:.04f}%\".format(100 * num_correct / ((i + 1) * batch_size)),\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            num_correct=num_correct,\n",
        "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "        \n",
        "        # Another couple things you need for FP16. \n",
        "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        scaler.update() # This is something added just for FP16\n",
        "\n",
        "        scheduler.step() # We told scheduler T_max that we'd call step() (len(train_loader) * epochs) many times.\n",
        "\n",
        "        batch_bar.update() # Update tqdm bar\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "    \n",
        "    if epoch%5 == 0:\n",
        "        torch.save(model.state_dict(), '/home/ubuntu/model_state.pth')\n",
        "\n",
        "    # You can add validation per-epoch here if you would like\n",
        "    for i, (x, y) in enumerate(val_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x = x.cuda()\n",
        "        y = y.cuda()\n",
        "\n",
        "        with torch.cuda.amp.autocast():     \n",
        "            val_outputs = model(x)\n",
        "        val_num_correct += int((torch.argmax(val_outputs, axis=1) == y).sum())\n",
        "        \n",
        "    if val_num_correct/(len(val_loader)*batch_size) > .86:\n",
        "        break\n",
        "    \n",
        "\n",
        "        \n",
        "\n",
        "    print(\"Epoch {}/{}: Train Acc {:.04f}%, Train Loss {:.04f}, Learning Rate {:.04f}, Val Acc {:.04f}%\".format(\n",
        "        epoch + 1,\n",
        "        epochs,\n",
        "        100 * num_correct / (len(train_loader) * batch_size),\n",
        "        float(total_loss / len(train_loader)),\n",
        "        float(optimizer.param_groups[0]['lr']),\n",
        "        100 * val_num_correct / (len(val_loader)*batch_size)))"
      ],
      "metadata": {
        "id": "imMNXTAW7pQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model.load_state_dict(torch.load('/home/ubuntu/model_state.pth'))\n",
        "model.eval()\n",
        "batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "num_correct = 0\n",
        "for i, (x, y) in enumerate(val_loader):\n",
        "\n",
        "    x = x.cuda()\n",
        "    y = y.cuda()\n",
        "    print(x.size())\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(x)\n",
        "\n",
        "    num_correct += int((torch.argmax(outputs, axis=1) == y).sum())\n",
        "    batch_bar.set_postfix(acc=\"{:.04f}%\".format(100 * num_correct / ((i + 1) * batch_size)))\n",
        "\n",
        "    batch_bar.update()\n",
        "    \n",
        "batch_bar.close()\n",
        "print(\"Validation: {:.04f}%\".format(100 * num_correct / len(val_dataset)))"
      ],
      "metadata": {
        "id": "cERC5Sv-7pS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationTestSet(Dataset):\n",
        "    # It's possible to load test set data using ImageFolder without making a custom class.\n",
        "    # See if you can think it through!\n",
        "\n",
        "    def __init__(self, data_dir, transforms):\n",
        "        self.data_dir = data_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # This one-liner basically generates a sorted list of full paths to each image in data_dir\n",
        "        self.img_paths = list(map(lambda fname: osp.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.transforms(Image.open(self.img_paths[idx]))"
      ],
      "metadata": {
        "id": "2fvXzXJa7pVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = ClassificationTestSet(TEST_DIR, ttf.Compose(val_transforms))\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
        "                         drop_last=False, num_workers=1)"
      ],
      "metadata": {
        "id": "sZ5ERp2R7pXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "batch_bar = tqdm(total=len(test_loader), dynamic_ncols=True, position=0, leave=False, desc='Test')\n",
        "\n",
        "res = []\n",
        "for i, (x) in enumerate(test_loader):\n",
        "    optimizer.zero_grad()\n",
        "    x = x.cuda()\n",
        "\n",
        "    with torch.cuda.amp.autocast():\n",
        "        outputs = model(x)\n",
        "\n",
        "    pred_y = torch.argmax(outputs, axis=1)\n",
        "\n",
        "    res.extend(pred_y.data.cpu().numpy())\n",
        "\n",
        "    # TODO: Finish predicting on the test set.\n",
        "    \n",
        "\n",
        "    batch_bar.update()\n",
        "    \n",
        "batch_bar.close()\n",
        "\n",
        "with open(\"submission.csv\", \"w\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"id\", \"label\"])\n",
        "    for num in range(len(res)):\n",
        "        id_num = str(num)\n",
        "        id = id_num.zfill(5)\n",
        "        id = id + '.jpg'\n",
        "        writer.writerow([id, res[num]])"
      ],
      "metadata": {
        "id": "CEMmM1ZP7paA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"classification_early_submission.csv\", \"w+\") as f:\n",
        "    f.write(\"id,label\\n\")\n",
        "    for i in range(len(test_dataset)):\n",
        "        f.write(\"{},{}\\n\".format(str(i).zfill(6) + \".jpg\", res[i]))"
      ],
      "metadata": {
        "id": "4010Oc367pcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit -c face-recognition-slack -f classification_early_submission.csv -m \"Initial submission\""
      ],
      "metadata": {
        "id": "MfZ2NYL27pep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit -c face-recognition-slack -f submission.csv -m \"Initial submission\""
      ],
      "metadata": {
        "id": "4KpgjmQk7phR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls verification/verification/dev | wc -l\n",
        "!cat verification/verification/verification_dev.csv | wc -l"
      ],
      "metadata": {
        "id": "f3piJR537pjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VerificationDataset(Dataset):\n",
        "    def __init__(self, data_dir, transforms):\n",
        "        self.data_dir = data_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # This one-liner basically generates a sorted list of full paths to each image in data_dir\n",
        "        self.img_paths = list(map(lambda fname: osp.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # We return the image, as well as the path to that image (relative path)\n",
        "        return self.transforms(Image.open(self.img_paths[idx])), osp.relpath(self.img_paths[idx], self.data_dir)"
      ],
      "metadata": {
        "id": "hmxUBXuf7poE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_veri_dataset = VerificationDataset(osp.join(DATA_DIR, \"verification/verification/dev\"),\n",
        "                                       ttf.Compose(val_transforms))\n",
        "val_ver_loader = torch.utils.data.DataLoader(val_veri_dataset, batch_size=batch_size, \n",
        "                                             shuffle=False, num_workers=1)"
      ],
      "metadata": {
        "id": "I7oxFp037pqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "feats_dict = dict()\n",
        "for batch_idx, (imgs, path_names) in tqdm(enumerate(val_ver_loader), total=len(val_ver_loader), position=0, leave=False):\n",
        "    imgs = imgs.cuda()\n",
        "    print(imgs[0].size())\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Note that we return the feats here, not the final outputs\n",
        "        # Feel free to try the final outputs too!\n",
        "        #print(imgs)\n",
        "        #print(path_names)\n",
        "        #print(batch_idx)\n",
        "        #print(imgs, path_names)\n",
        "        #for item in (imgs, path_names):\n",
        "            #for i in item:\n",
        "            #    print(len(i))\n",
        "            #print(img)\n",
        "            #print(path_name)\n",
        "        #    img = item[0]\n",
        "        feats = model(imgs, return_feats=False)\n",
        "        feats = F.gelu(feats)\n",
        "        print(len(feats))\n",
        "        for i in range(len(feats)):\n",
        "            feat = feats[i]\n",
        "            #img = imgs[i]\n",
        "            path_name = path_names[i]\n",
        "\n",
        "            #print(len(feats))\n",
        "            #path_name = item[1]\n",
        "            #feats = model(img, return_feats=True) \n",
        "\n",
        "            feats_dict[path_name] = feat\n",
        "            #print(len(feat))\n",
        "    \n",
        "    # TODO: Now we have features and the image path names. What to do with them?\n",
        "    # Hint: use the feats_dict somehow."
      ],
      "metadata": {
        "id": "bB7yC63Q72_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We use cosine similarity between feature embeddings.\n",
        "# TODO: Find the relevant function in pytorch and read its documentation.\n",
        "similarity_metric = nn.CosineSimilarity(dim=0)\n",
        "\n",
        "val_veri_csv = osp.join(DATA_DIR, \"verification/verification/verification_dev.csv\")\n",
        "\n",
        "\"\"\"\n",
        "val_veri_dataset = VerificationDataset(osp.join(DATA_DIR, \"verification/verification/dev\"),\n",
        "                                       ttf.Compose(val_transforms))\n",
        "val_ver_loader = torch.utils.data.DataLoader(val_veri_dataset, batch_size=batch_size, \n",
        "                                             shuffle=False, num_workers=1)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Now, loop through the csv and compare each pair, getting the similarity between them\n",
        "pred_similarities = []\n",
        "gt_similarities = []\n",
        "for line in tqdm(open(val_veri_csv).read().splitlines()[1:], position=0, leave=False): # skip header\n",
        "    img_path1, img_path2, gt = line.split(\",\")\n",
        "    #print(img_path1)\n",
        "    #print(img_path1.split('/')[1])\n",
        "    img_path1 = img_path1.split('/')[1]\n",
        "    img_path2 = img_path2.split('/')[1]\n",
        "    #print(feats_dict[img_path1])\n",
        "    #print(feats_dict[img_path2])\n",
        "\n",
        "    # TODO: Use the similarity metric\n",
        "    # How to use these img_paths? What to do with the features?\n",
        "    similarity = similarity_metric(feats_dict[img_path1], feats_dict[img_path2])\n",
        "    #print(float(similarity.cpu()))\n",
        "\n",
        "    gt_similarities.append(int(gt))\n",
        "    pred_similarities.append(similarity.cpu())\n",
        "#pred_similarities = pred_similarities\n",
        "#gt_similarities = gt_similarities.numpy()\n",
        "#pred_similarities = np.array(pred_similarities)\n",
        "#gt_similarities = np.array(gt_similarities)\n",
        "print(gt_similarities[0])\n",
        "print(pred_similarities[0])\n",
        "print(\"AUC:\", roc_auc_score(gt_similarities, pred_similarities))"
      ],
      "metadata": {
        "id": "JbVsQmI173Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_veri_dataset = VerificationDataset(osp.join(DATA_DIR, \"verification/verification/test\"),\n",
        "                                        ttf.Compose(val_transforms))\n",
        "test_ver_loader = torch.utils.data.DataLoader(test_veri_dataset, batch_size=batch_size, \n",
        "                                              shuffle=False, num_workers=1)"
      ],
      "metadata": {
        "id": "wBiT25CT73EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "feats_dict = dict()\n",
        "for batch_idx, (imgs, path_names) in tqdm(enumerate(test_ver_loader), total=len(test_ver_loader), position=0, leave=False):\n",
        "    imgs = imgs.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Note that we return the feats here, not the final outputs\n",
        "        # Feel free to try to final outputs too!\n",
        "\n",
        "        feats = model(imgs, return_feats=False)\n",
        "        feats = F.gelu(feats)\n",
        "\n",
        "        for i in range(len(feats)):\n",
        "            feat = feats[i]\n",
        "\n",
        "            path_name = path_names[i]\n",
        "\n",
        "            feats_dict[path_name] = feat\n",
        "    \n",
        "    # TODO: Now we have features and the image path names. What to do with them?\n",
        "    # Hint: use the feats_dict somehow."
      ],
      "metadata": {
        "id": "jBPVBvx-73Ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We use cosine similarity between feature embeddings.\n",
        "# TODO: Find the relevant function in pytorch and read its documentation.\n",
        "similarity_metric = nn.CosineSimilarity(dim=0)\n",
        "val_veri_csv = osp.join(DATA_DIR, \"verification/verification/verification_test.csv\")\n",
        "\n",
        "# Now, loop through the csv and compare each pair, getting the similarity between them\n",
        "pred_similarities = []\n",
        "for line in tqdm(open(val_veri_csv).read().splitlines()[1:], position=0, leave=False): # skip header\n",
        "    img_path1, img_path2= line.split(\",\")\n",
        "    #print(img_path1)\n",
        "    #print(img_path1.split('/')[1])\n",
        "    img_path1 = img_path1.split('/')[1]\n",
        "    img_path2 = img_path2.split('/')[1]\n",
        "    #print(feats_dict[img_path1])\n",
        "    #print(feats_dict[img_path2])\n",
        "\n",
        "    # TODO: Use the similarity metric\n",
        "    # How to use these img_paths? What to do with the features?\n",
        "    similarity = similarity_metric(feats_dict[img_path1], feats_dict[img_path2])\n",
        "    #print(float(similarity.cpu()))\n",
        "\n",
        "    pred_similarities.append(similarity.cpu())\n",
        "#pred_similarities = pred_similarities\n",
        "#gt_similarities = gt_similarities.numpy()\n",
        "#pred_similarities = np.array(pred_similarities)\n",
        "#gt_similarities = np.array(gt_similarities)\n",
        "\n",
        "print(pred_similarities[0])\n"
      ],
      "metadata": {
        "id": "MisnQd7_73Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"verification_early_submission.csv\", \"w+\") as f:\n",
        "    f.write(\"id,match\\n\")\n",
        "    for i in range(len(pred_similarities)):\n",
        "        f.write(\"{},{}\\n\".format(i, pred_similarities[i]))"
      ],
      "metadata": {
        "id": "69nu5nfo73Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit -c face-verification-slack -f verification_early_submission.csv -m \"first_submission\""
      ],
      "metadata": {
        "id": "Dvdp5ym073NP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CqeO8NZZ73QG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rLpZLEbo73S_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IKA2F_pj73WQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}